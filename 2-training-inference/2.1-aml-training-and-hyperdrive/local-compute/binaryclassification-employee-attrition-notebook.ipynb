{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn binary classification model training on local notebook\n",
    "\n",
    "## Plus Azure ML Dataset converted to Pandas DataFrames\n",
    "\n",
    "_**The code is plain vanilla Scikit-Learn training/creation of a Binary classification model.**_\n",
    "\n",
    "_**Azure ML is only used to gather original data from an AML Dataset.**_\n",
    "\n",
    "_**This notebook can run on a local PC or on any Azure ML Compute Instance or Azure ML VM.**_\n",
    "\n",
    "This code is the baseline for the next labs/notebooks in the Workshop moving to AML remote compute, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries to use in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core import Environment\n",
    "\n",
    "# Check versions\n",
    "import azureml.core\n",
    "import sklearn\n",
    "import joblib\n",
    "import pandas\n",
    "\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
    "print('scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "print('joblib version is {}.'.format(joblib.__version__))\n",
    "print('pandas version is {}.'.format(pandas.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace to load Tabular Datasets and log info from local training into AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to an existing Azure ML Workshop in order to use Azure ML Datasets and Runs Logging into AML\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the IBM employee attrition data created before\n",
    "\n",
    "**Note:** as you are now accessing the workspace, the Notebook needs to be authenticated for access through device authentication. Hence, you will be prompted with a device login like so: \n",
    "\n",
    "    Performing interactive authentication. Please follow the instructions on the terminal.\n",
    "    To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code ARQPFR4B4 to authenticate.\n",
    "    \n",
    "Please follow these instructions in a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the IBM employee attrition dataset from the workspace\n",
    "\n",
    "attritionData = ws.datasets['IBM-Employee-Attrition'].to_pandas_dataframe()\n",
    "attritionData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Employee count as all values are 1 and hence attrition is independent of this feature\n",
    "attritionData = attritionData.drop(['EmployeeCount'], axis=1)\n",
    "\n",
    "# Dropping Employee Number since it is merely an identifier\n",
    "attritionData = attritionData.drop(['EmployeeNumber'], axis=1)\n",
    "\n",
    "attritionData = attritionData.drop(['Over18'], axis=1)\n",
    "\n",
    "# Since all values are 80\n",
    "attritionData = attritionData.drop(['StandardHours'], axis=1)\n",
    "target = attritionData[\"Attrition\"]\n",
    "\n",
    "attritionXData = attritionData.drop(['Attrition'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in Train and Test datasets (DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(attritionXData, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the categorical and numerical column names in separate lists\n",
    "categorical = []\n",
    "for col, value in attritionXData.iteritems():\n",
    "    if value.dtype == 'object':\n",
    "        categorical.append(col)\n",
    "        \n",
    "numerical = attritionXData.columns.difference(categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform raw features\n",
    "We can explain raw features by either using a sklearn.compose.ColumnTransformer or a list of fitted transformer tuples. The cell below uses sklearn.compose.ColumnTransformer. In case you want to run the example with the list of fitted transformer tuples, comment the cell below and uncomment the cell that follows after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data processing pipelines (Scikit-Learn pipelines)\n",
    "**Note:** This code uses Scikit-Learn pipelines. This is not related to AML Pipelines. Different concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# We create the transformations pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transforms_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical),\n",
    "        ('cat', categorical_transformer, categorical)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add classifier algorithm (SVC: Support Vector Classifier) to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to Scikit-Learn transformations pipeline.\n",
    "# Now we have a full Scikit-Learn prediction pipeline.\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', transforms_pipeline),\n",
    "                      ('classifier', SVC(kernel='linear', C = 1.0, probability=True))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML Experiment, run and log just for logging info while training locally in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Get an experiment object from AML\n",
    "experiment = Experiment(workspace=ws, name=\"local-train-notebook-aml-log\")\n",
    "\n",
    "# Create a run object in the experiment\n",
    "run =  experiment.start_logging()\n",
    "\n",
    "# Log the algorithm parameter C to the run\n",
    "run.log('C', 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the SVM (Support Vector Machine) Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions and calculate Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make Multiple Predictions\n",
    "y_predictions = model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log metric and model into the AML run definition\n",
    "### Note that training is local, we just use the run definition to log information about the run/training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the Mean Squared Error to the notebook and to the run\n",
    "run.log('accuracy', accuracy)\n",
    "\n",
    "# Save the model to the outputs directory for capture\n",
    "model_file_name = 'outputs/model.pkl'\n",
    "\n",
    "joblib.dump(value = model, filename = model_file_name)\n",
    "\n",
    "# upload the model file explicitly into artifacts \n",
    "run.upload_file(name = model_file_name, path_or_stream = model_file_name)\n",
    "\n",
    "# Complete the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the experiment run and its logged info in Azure ML Workspace\n",
    "Now, you should go to your AML Workspace and check the information logged for this run, such as the accuracy, hyper-parameters and any other info you logged for the experiment run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predictions)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "# Show confusion matrix in a separate window\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Prediction\n",
    "instance_num = 6\n",
    "# Get the prediction for the first member of the test set and explain why model made that prediction\n",
    "prediction_value = model.predict(x_test)[instance_num]\n",
    "\n",
    "print(\"One Prediction:\")\n",
    "print(prediction_value)\n",
    "\n",
    "print(y_predictions[:20])\n",
    "\n",
    "x_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
