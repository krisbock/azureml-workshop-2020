{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn binary classification model\n",
    "\n",
    "## Remote train via Azure ML Compute (AML Cluster) and optional HyperDrive (Hyper-parameter tunning with multiple child runs)\n",
    "\n",
    "- This notebook showcases the creation of a ScikitLearn Binary classification model by remotely training on Azure ML Compute Target (AMLCompute Cluster)\n",
    "\n",
    "- It shows multiple ways of remote training like using a single Estimator, a ScriptRunConfig and hyper-parameter tunning with HyperDrive with multiple child trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will work in Jupyter out of the box, but not in JupyterLab\n",
    "# Hence we need to install the JupyterLab Widget Extension\n",
    "# Afterwards, restart Jupyter by opening Jupyter and hitting the Quit button, then re-login to JupyterLab\n",
    "!sudo -i jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check library versions\n",
    "\n",
    "This is important when interacting with different executions between remote compute environments (cluster) and the Compute Instance/VM/Laptop/etc. with the Jupyter Notebook.\n",
    "If not using the same versions you can have issues when creating .pkl files in the cluster and downloading them to load it in the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check versions\n",
    "import azureml.core\n",
    "import sklearn\n",
    "import joblib\n",
    "import pandas\n",
    "\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
    "print('scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "print('joblib version is {}.'.format(joblib.__version__))\n",
    "print('pandas version is {}.'.format(pandas.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and connect to AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "#ws = Workspace.from_config()\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
    "\n",
    "ws = Workspace(subscription_id=\"3c8972d9-f541-46b2-b70b-d81baba3595d\",\n",
    "                  resource_group=\"amlworkshop\",\n",
    "                  workspace_name=\"amlworkshop\",\n",
    "                  auth=interactive_auth)\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'classif-attrition-amlcompute'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to AmlCompute\n",
    "\n",
    "Azure Machine Learning Compute is managed compute infrastructure that allows the user to easily create single to multi-node compute of the appropriate VM Family. It is created **within your workspace region** and is a resource that can be used by other users in your workspace. It autoscales by default to the `max_nodes`, when a job is submitted, and executes in a containerized environment packaging the dependencies as specified by the user. \n",
    "\n",
    "Since it is managed compute, job scheduling and cluster management are handled internally by Azure Machine Learning service. \n",
    "\n",
    "For more information on Azure Machine Learning Compute, please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute)\n",
    "\n",
    "**Note**: As with other Azure services, there are limits on certain resources (for eg. AmlCompute quota) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create project directory and copy the training script into the project directory\n",
    "\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "project_folder = './classif-attrition-amlcompute'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "\n",
    "# Copy the training script into the project directory\n",
    "shutil.copy('train.py', project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect or Create a Remote AML compute cluster\n",
    "\n",
    "Try to use the compute target you had created before (make sure you provide the same name here in the variable `cpu_cluster_name`).\n",
    "If not available, create a new cluster from the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define remote compute target to use\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "     found = True\n",
    "     print('Found existing training cluster.')\n",
    "     # Get existing cluster - Method 1:\n",
    "     aml_remote_compute = cts[amlcompute_cluster_name]\n",
    "     # Alternative - Method 2:\n",
    "     # aml_remote_compute = ComputeTarget(ws, amlcompute_cluster_name)\n",
    "    \n",
    "if not found:\n",
    "     print('Creating a new training cluster...')\n",
    "     provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D13_V2\", # for GPU, use \"STANDARD_NC12\"\n",
    "                                                                 #vm_priority = 'lowpriority', # optional\n",
    "                                                                 max_nodes = 8)\n",
    "     # Create the cluster.\n",
    "     aml_remote_compute = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "aml_remote_compute.wait_for_completion(show_output = True, min_node_count = 0, timeout_in_minutes = 20)\n",
    "    \n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the AML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_dataset = ws.datasets['IBM-Employee-Attrition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment\n",
    "\n",
    "#### Optionally list all the available environments and packages in your AML Workspace\n",
    "\n",
    "- Environments specify the Python packages, environment variables, and software settings around your training and scoring scripts.\n",
    "- They also specify run times (Python, Spark, or Docker).\n",
    "- The environments are managed and versioned entities within your Machine Learning workspace that enable reproducible, auditable, and portable machine learning workflows across a variety of compute targets.\n",
    "- You can also persist environements file-based in `yaml` or `json`.\n",
    "\n",
    "For more details, see [this link](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "envs = Environment.list(workspace=ws)\n",
    "\n",
    "# List Environments and packages in my workspace\n",
    "for env in envs:\n",
    "    if env.startswith(\"AzureML\"):\n",
    "        print(\"Name\",env)\n",
    "        #print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n",
    "        \n",
    "# Use curated environment from AML named \"AzureML-Tutorial\"\n",
    "curated_environment = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\n",
    "\n",
    "# Custom environment: Environment.get(workspace=ws,name=\"myenv\",version=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure & Run using ScriptRunConfig & Environment \n",
    "\n",
    "The easiest option for running our code is using a `ScriptRunConfig` with an environement. This is especially easy to use for single node training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add training script to run config\n",
    "from azureml.core import ScriptRunConfig, RunConfiguration, Experiment\n",
    "\n",
    "script_runconfig = ScriptRunConfig(source_directory=project_folder, \n",
    "                            script=\"train.py\",\n",
    "                            arguments=[aml_dataset.as_named_input('attrition')]\n",
    "                           )\n",
    "\n",
    "# Attach compute target to run config\n",
    "script_runconfig.run_config.target = aml_remote_compute\n",
    "# runconfig.run_config.target = \"local\"\n",
    "\n",
    "# Attach environment to run config\n",
    "script_runconfig.run_config.environment = curated_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Option A) Run the experiment with a single ScriptRunConfig and single run\n",
    "\n",
    "This will train the model with fixed parameters on a single node. If you want to use hyperparameter tuning, see option C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the Experiment Run to the AML Compute \n",
    "run = experiment.submit(script_runconfig)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment will now show up in the AzureML studio UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Show interactive widget for monitoring the run\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get log results upon completion\n",
    "Model training and monitoring happen in the background. Wait until the model has finished training before you run more code. Use `wait_for_completion()` to show when the model training is finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will wait until the run finished\n",
    "run.wait_for_completion(show_output=True)  # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Option B) Configure and Run with Intelligent hyperparameter tuning (HyperDrive using Estimator)\n",
    "\n",
    "**IMPORTANT:** You need to have created the ScriptRunConfig in the previous steps (Without submitting the experiment). \n",
    "\n",
    "The adjustable parameters that govern the training process are referred to as the **hyperparameters** of the model. The goal of hyperparameter tuning is to search across various hyperparameter configurations and find the configuration that results in the best performance.\n",
    "\n",
    "To demonstrate how Azure Machine Learning can help you automate the process of hyperarameter tuning, we will launch multiple runs with different values for numbers in the sequence. First let's define the parameter space using random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a hyperparameter sweep\n",
    "First, we will define the hyperparameter space to sweep over. \n",
    "In this example we will use random sampling to try different configuration sets of hyperparameters to maximize our primary metric, Accuracy. More configuration details can be found [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, BayesianParameterSampling \n",
    "from azureml.train.hyperdrive import BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "    \n",
    "# solver{'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'\n",
    "# penalty{'l1', 'l2', 'elasticnet', 'none'}, default='l2' --- Note that some penalty parameters are not supported by some algorithms..\n",
    "param_sampling = RandomParameterSampling( {\n",
    "    \"--solver\": choice('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n",
    "    \"--penalty\": choice('l2')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Details on Scikit-Learn LogisticRegression hyper-parameters:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define an early termination policy. The BanditPolicy basically states to check the job every 2 iterations. If the primary metric (defined later) falls outside of the top 10% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
    "# Note that early termination policy is currently NOT supported with Bayesian sampling\n",
    "# Check here for recommendations on the multiple policies:\n",
    "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters#picking-an-early-termination-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to configure a run configuration object, and specify the primary metric 'Accuracy' that's recorded in your training runs. \n",
    "If you go back to visit the training script, you will notice that this value is being logged. \n",
    "We also want to tell the service that we are looking to maximizing this value. \n",
    "We also set the number of samples to 20, and maximal concurrent job to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that in this case when using HyperDrive, the original ScriptRunConfig's parameters are not used but the HyperDrive parameters...\n",
    "hyperdrive_config = HyperDriveConfig(run_config=script_runconfig, \n",
    "                                     # estimator=estimator,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     policy=early_termination_policy,\n",
    "                                     primary_metric_name='Accuracy',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=5,\n",
    "                                     max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, launch the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)\n",
    "\n",
    "# Check here how to submit the hyperdrive run as a step of an AML Pipeline:\n",
    "# https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor HyperDrive run\n",
    "\n",
    "Monitor the progress of the runs with the Jupyter widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get details for debugging:\n",
    "# RunDetails(run).get_widget_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get log results upon completion\n",
    "Model training and monitoring happen in the background. Wait until the model has finished training before you run more code. Use wait_for_completion to show when the model training is finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and get the best model found by HyperDrive¶ \n",
    "When all jobs finish, we can find out the one that has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_details()['runDefinition']['arguments'])\n",
    "# print(best_run.get_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy 'best_run' to 'run' to re-use the same code also used without HyperDrive\n",
    "run = best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display run metrics results\n",
    "You now have a model trained on a remote cluster. Retrieve the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See files associated with the run\n",
    "\n",
    "We can either see the logs for each run in the AzureML studio UI or download them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_file_names())\n",
    "\n",
    "run.download_file('azureml-logs/70_driver_log.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "Once you've trained the model, you can save and register it to your workspace. Model registration lets you store and version your models in your workspace to simplify model management and deployment.\n",
    "\n",
    "Running the following code will register the model to your workspace, and will make it available to reference by name in remote compute contexts or deployment scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_reg = run.register_model(model_name='classif-empl-attrition-aml-comp',    # Name of the registered model in your workspace.\n",
    "                               description='Binary classification model for employees attrition',\n",
    "                               model_path='outputs/classif-empl-attrition.pkl', # Local file to upload and register as a model.\n",
    "                               model_framework=Model.Framework.SCIKITLEARN,     # Framework used to create the model.\n",
    "                               model_framework_version='0.22.2.post1',                # Version of scikit-learn used to create the model.\n",
    "                               tags={'ml-task': \"binary-classification\", 'business-area': \"HR\"},\n",
    "                               properties={'joblib-version': \"0.14.0\", 'pandas-version': \"0.24.1\"},\n",
    "                               sample_input_dataset=aml_dataset\n",
    "                              )\n",
    "model_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Scikit-Learn model pickle file from the run (Option A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model .pkl file to local (Using the 'run' object)\n",
    "run.download_file('outputs/classif-empl-attrition.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Scikit-Learn model pickle file from the model registry (Option B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Model.get_model_path('classif-empl-attrition-aml-comp', _workspace=ws))\n",
    "\n",
    "model_from_registry = Model(ws,'classif-empl-attrition-aml-comp')\n",
    "model_from_registry.download(target_dir='.', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try model predictions in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model into memory\n",
    "model = joblib.load('classif-empl-attrition.pkl')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and test datasets from .pkl files\n",
    "\n",
    "# Download the test datasets to local\n",
    "run.download_file('outputs/x_test.pkl')\n",
    "run.download_file('outputs/y_test.pkl')\n",
    "\n",
    "# Load the test datasets into memory\n",
    "x_test = joblib.load('x_test.pkl')\n",
    "y_test = joblib.load('y_test.pkl')\n",
    "\n",
    "# joblib\n",
    "# https://joblib.readthedocs.io/en/latest/installing.html\n",
    "# https://joblib.readthedocs.io/en/latest/generated/joblib.load.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions and calculate Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make Multiple Predictions\n",
    "y_predictions = model.predict(x_test)  # .predict(X[0:1])\n",
    "\n",
    "y_predictions\n",
    "\n",
    "print('Accuracy:')\n",
    "accuracy_score(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predictions)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "# Show confusion matrix in a separate window\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Prediction\n",
    "instance_num = 6\n",
    "# Get the prediction for the first member of the test set and explain why model made that prediction\n",
    "prediction_value = model.predict(x_test)[instance_num]\n",
    "\n",
    "print(\"One Prediction: \")\n",
    "print(prediction_value)\n",
    "\n",
    "print(\"20 Predictions: \")\n",
    "print(y_predictions[:20])\n",
    "\n",
    "x_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(5)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('azureml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "04668ec82c5a22c2f4541de1503ac33d35cb2501de93142d187819277643c9f9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}